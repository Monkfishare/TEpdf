<html lang="en"><meta name="viewport" content="width=device-width, initial-scale=1" /><head><link rel="stylesheet" href="../init.css"></head><body><section class="css-qm1vln e1m3q8rc0"><div class="css-b4a1pi elvs37n0"><span class="css-1te5c83 e11l78dl0"><a data-analytics="sidebar:section" href="/business/"><span>Business</span></a></span><span class="css-1mdqtqm e11bnqe00"> | <!-- -->Of evils and evals</span></div><h1 class="css-fk78xg e1t4u3eq0">The world wants to regulate AI, but does not quite know how</h1><h2 class="css-o55d57 elmr55g0">There is disagreement over what is to be policed, how and by whom</h2></section><img id="myImg" src="../image/20231028_WBD003.jpg" width="auto" height="200"><section class="css-12k3cm0 e1ey6m751" data-body-id="cp2"><div class="css-1746s8a e1ey6m752"><div class="css-1lm38nn e50l3u40"><figure><div><figcaption>Listen to this story.</figcaption> <span class="css-1fs0t47 ed4dtdz0">Enjoy more audio and podcasts on<!-- --> <a href="https://economist-app.onelink.me/d2eC/bed1b25" id="audio-ios-cta" rel="noreferrer" target="_blank">iOS</a> <!-- -->or<!-- --> <a href="https://economist-app.onelink.me/d2eC/7f3c199" id="audio-android-cta" rel="noreferrer" target="_blank">Android</a>.</span></div><audio class="react-audio-player" controls="" controlslist="nodownload" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/057%20Business%20-%20Artificial%20intelligence-3e3a659c20bf42852a871582b89d0c7f.mp3" title="The world wants to regulate AI, but does not quite know how"><p>Your browser does not support the &lt;audio&gt; element.</p></audio><div class="css-1h1me4y e50l3u41"><div class="css-vu6x35 ebr52qc0"></div></div></figure></div><p class="css-1hno3qs e190yofl0" data-component="paragraph"><span data-caps="initial">T</span><small>he venue</small> will be picturesque: a 19th-century pile north of London that during the second world war was home to <a href="https://www.economist.com/science-and-technology/2019/07/18/alan-turing-a-computing-pioneer-will-feature-on-britains-ps50-notes">Alan Turing</a>, his code-breaking crew and the first programmable digital computer. The attendees will be an elite bunch of 100 world leaders and t<a href="https://www.economist.com/business/2023/05/25/why-tech-giants-want-to-strangle-ai-with-red-tape">ech executives</a>. And the question they will strive to answer is epochal: how to ensure that <a href="https://www.economist.com/films/2023/09/14/can-ai-be-controlled">artificial intelligence</a> neither becomes a tool of unchecked malfeasance nor turns against humanity.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">The “<small>AI</small> Safety Summit”, which the British government is hosting on November 1st and 2nd at Bletchley Park, appears destined for the history books. And it may indeed one day be seen as the first time global power-brokers sat down to discuss seriously what to do about a technology that may change the world. As Jonathan Black, one of the organisers, observed, in contrast to other big policy debates, such as climate change, “there is a lot of good will” but “we still don’t know what the right answer is.”</p><div class="adComponent_advert__V79Pp adComponent_incontent__Bxd2J adComponent_hidden___o_ZB css-0 e1cbnkh80"><div><div class="adComponent_adcontainer__dO1Zm" id="econ-1"></div></div></div><p class="css-1hno3qs e190yofl0" data-component="paragraph">Efforts to rein in <small>AI </small>abound. Negotiations in Brussels entered a pivotal stage on October 25th as officials grappled to finalise the European Union’s ambitious <small>AI </small>act by the end of the year. In the days leading up to Britain’s summit or shortly thereafter, the White House is expected to issue an executive order on <small>AI</small>. The <small>G</small>7 club of rich democracies will this autumn start drafting a code of conduct for <small>AI </small>firms. China, for its part, on October 18th unveiled a “Global AI Governance Initiative”.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">The momentum stems from an unusual political economy. Incentives to act, and act together, are strong. For starters, <small>AI </small>is truly a global technology. Large language models (<small>LLM</small>s), which power eerily humanlike services such as Chat<small>GPT</small>, travel easily. Some can be run on a laptop. It is of little use to tighten the screws on <small>AI</small> in some countries if they remain loose in others. Voters may be in favour. More than half of Americans “are more concerned than excited” about the use of <small>AI</small>, according to polling by the Pew Research Centre.</p><h2 class="css-8p8dhl e8f47sz0">The Beijing effect</h2><p class="css-1hno3qs e190yofl0" data-component="paragraph">Regulatory rivalry is adding more urgency. Europe’s <small>AI </small>act is intended in part to cement the bloc’s role as the setter of global digital standards. The White House would love to forestall such a “Brussels effect”. Neither the <small>EU </small>nor<small> </small>America wants to be outdone by China, which has already adopted several <small>AI </small>laws. They were cross with the British government for inviting China to the summit—never mind that without it, any regulatory regime would not be truly global. (China may actually show up, even if its interest is less to protect humanity than the Communist Party.)</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Another driver of <small>AI</small>-rulemaking diplomacy is even more surprising: the model-makers themselves. In the past the technology industry mostly opposed regulation. Now giants such as Alphabet and Microsoft, and <small>AI </small>darlings like Anthropic and Open<small>AI</small>, which created Chat<small>GPT</small>, lobby for it. Companies fret that unbridled competition will push them to act recklessly by releasing models that could easily be abused or start developing minds of their own. That would really land them in hot water.</p><div class="adComponent_advert__V79Pp adComponent_incontent__Bxd2J adComponent_hidden___o_ZB css-0 e1cbnkh80"><div><div class="adComponent_adcontainer__dO1Zm" id="econ-2"></div></div></div><p class="css-1hno3qs e190yofl0" data-component="paragraph">The will to act is there, in other words. What is not there is “anything approaching consensus as to what the problems are that we need to govern, let alone how it is that we ought to govern them”, says Henry Farrell of Johns Hopkins University. Three debates stand out. What should the world worry about? What should any rules target? And how should they be enforced?</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Start with the goals of regulation. These are hard to set because <small>AI </small>is evolving rapidly. Hardly a day passes without a startup coming up with something new. Even the developers of <small>LLM</small>s cannot say for sure what capabilities these will exhibit. This makes it crucial to have tests that can gauge how risky they might be—something that is still more art than science. Without such “evals” (short for evaluations), it will be hard to check whether a model is complying with any rules.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Tech companies may back regulation, but want it to be narrow and target only extreme risks. At a Senate hearing in Washington in July, Dario Amodei, Anthropic’s chief executive, warned that <small>AI </small>models will in a few years be able to provide all the information needed to build bioweapons, enabling “many more actors to carry out large scale biological attacks”. Similar dire forecasts are being made about cyber-weapons. Earlier this month Gary Gensler, chairman of America’s Securities and Exchange Commission, predicted that an <small>AI</small>-engineered financial crisis was “nearly unavoidable” without swift intervention.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Others argue that these speculative risks distract from other threats, such as undermining the democratic process. At an earlier Senate hearing Gary Marcus, a noted <small>AI </small>sceptic, opened his testimony with a snippet of breaking news written by <small>GPT</small>-4, Open<small>AI</small>’s top model. It convincingly alleged that parts of Congress were “secretly manipulated by extraterrestrial entities”. “We should all be deeply worried,” Mr Marcus argued, “about systems that can fluently confabulate.”</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">The debate over what exactly to regulate will be no easier to resolve. Tech firms mostly suggest limiting scrutiny to the most powerful “frontier” models. Microsoft, among others, has called for a licensing regime requiring firms to register models that exceed certain performance thresholds. Other proposals include controlling the sale of powerful chips used to train <small>LLM</small>s and mandating that cloud-computing firms inform authorities when customers train frontier models.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Most firms also agree it is models’ applications, rather than the models themselves, that ought to be regulated. Office software? Light touch. Health-care <small>AI</small>? Stringent rules. Facial recognition in public spaces? Probably a no-go. The advantage of such use-based regulation is that existing laws would mostly suffice. The <small>AI </small>developers warn that broader and more intrusive rules would slow down innovation.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Until last year America, Britain and the <small>EU </small>seemed to agree on this risk-based approach. The breathtaking rise of <small>LLM</small>s since the launch of Chat<small>GPT</small> a year ago is giving them second thoughts. The <small>EU</small> is now wondering whether the models themselves need to be overseen, after all. The European Parliament wants model-makers to test <small>LLM</small>s for potential impact on everything from human health to human rights. It insists on getting information about the data on which the models are trained. Canada has a harder-edged “Artificial Intelligence and Data Act” in its parliamentary works. Brazil is discussing something similar. In America, President Joe Biden’s forthcoming executive order is also expected to include some tougher rules. Even Britain may revisit its hands-off approach.</p><div class="adComponent_advert__V79Pp adComponent_incontent__Bxd2J adComponent_hidden___o_ZB css-0 e1cbnkh80"><div><div class="adComponent_adcontainer__dO1Zm" id="econ-3"></div></div></div><p class="css-1hno3qs e190yofl0" data-component="paragraph">These harder regulations would be a change from non-binding codes of conduct, which have hitherto been the preferred approach. Last summer the White House negotiated a set of “voluntary commitments”, which 15 model-makers have now signed. The firms agreed to have their models internally and externally tested before release and to share information about how they manage <small>AI </small>risks.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Then there is the question of who should do the regulating. America and Britain think existing government agencies can do most of the job. The <small>EU </small>wants to create a new regulatory body. Internationally, a few tech executives now call for the creation of something akin to the Intergovernmental Panel on Climate Change (<small>IPCC</small>), which the <small>UN </small>tasks with keeping abreast of research into global warming and with developing ways to gauge its impact.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Given all these open questions, it comes as no surprise that the organisers of the London summit do not sound that ambitious. It should mainly be thought of as “a conversation”, said Mr Black. Still, the not-so-secret hope is that it will yield a few tangible results, in particular on day two when only 20 or so of the most important corporate and world leaders remain in the room. They could yet endorse the White House’s voluntary commitments and recommend the creation of an <small>IPCC </small>for <small>AI</small> or even globalising Britain’s existing “Frontier <small>AI </small>Taskforce”.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Such an outcome would count as a success for Britain’s government. It would also speed up the more official efforts at global <small>AI </small>governance, such as the <small>G</small>7’s code of conduct. As such, it would be a useful first step. It will not be the last. <span class="ufinish">■</span></p><p class="css-1hno3qs e190yofl0" data-component="paragraph"><i>To stay on top of the biggest stories in business and technology, sign up to the <a href="https://www.economist.com/newsletters/the-bottom-line">Bottom Line</a>, our weekly subscriber-only newsletter.</i></p><p class="css-1hno3qs e190yofl0" data-component="paragraph"><i>Read more of our articles on <a href="https://www.economist.com/artificial-intelligence">artificial intelligence</a></i></p></div></section></body>
    <script>
    window.tedl = window.tedl || {};
    // Resize iframes on articles with interactives when they send a RESIZE message
    window.addEventListener('message', (event) => {
    if (event.data.type === 'RESIZE') {
    Array.prototype.forEach.call(document.getElementsByTagName('iframe'), function (element) {
    if (element.contentWindow === event.source) {
    const height = parseInt(event.data.payload.height, 10);
    const elementHeight = parseInt(element.style.height, 10);
    if (isNaN(elementHeight) | Math.abs(elementHeight - height) > 10){
        element.style.height = height + 'px';
    }
    // 
    console.log(elementHeight - height);
    }
    });
    }
    }, false);
    </script>
    </html>