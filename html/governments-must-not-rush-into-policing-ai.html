<html lang="en"><meta name="viewport" content="width=device-width, initial-scale=1" /><head><link rel="stylesheet" href="../init.css"></head><body><section class="css-qm1vln e1m3q8rc0"><div class="css-b4a1pi elvs37n0"><span class="css-1te5c83 e11l78dl0"><a data-analytics="sidebar:section" href="/leaders/"><span>Leaders</span></a></span><span class="css-1mdqtqm e11bnqe00"> | <!-- -->Think, then act</span></div><h1 class="css-fk78xg e1t4u3eq0">Governments must not rush into policing AI </h1><h2 class="css-o55d57 elmr55g0">A summit in Britain will focus on “extreme” risks. But no one knows what they look like </h2></section><img id="myImg" src="../image/20231028_LDD004.jpg" width="auto" height="200"><section class="css-12k3cm0 e1ey6m751" data-body-id="cp2"><div class="css-1746s8a e1ey6m752"><div class="css-1lm38nn e50l3u40"><figure><div><figcaption>Listen to this story.</figcaption> <span class="css-1fs0t47 ed4dtdz0">Enjoy more audio and podcasts on<!-- --> <a href="https://economist-app.onelink.me/d2eC/bed1b25" id="audio-ios-cta" rel="noreferrer" target="_blank">iOS</a> <!-- -->or<!-- --> <a href="https://economist-app.onelink.me/d2eC/7f3c199" id="audio-android-cta" rel="noreferrer" target="_blank">Android</a>.</span></div><audio class="react-audio-player" controls="" controlslist="nodownload" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/007%20Leaders%20-%20Artificial%20intelligence-2d5505392370b306e075b211a5fda489.mp3" title="Governments must not rush into policing AI "><p>Your browser does not support the &lt;audio&gt; element.</p></audio><div class="css-1h1me4y e50l3u41"><div class="css-vu6x35 ebr52qc0"></div></div></figure></div><p class="css-1hno3qs e190yofl0" data-component="paragraph"><span data-caps="initial">W</span><small>ill artificial</small> intelligence kill us all? Some technologists sincerely believe the answer is yes. In one nightmarish scenario, <small>AI</small> eventually outsmarts humanity and goes rogue, taking over computers and factories and filling the sky with killer drones. In another, large language models (<small>LLM</small>s) of the sort that power generative <small>ai</small>s like Chat<small>GPT</small> give bad guys the know-how to create devastating cyberweapons and deadly new pathogens.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">It is time to think hard about these doomsday scenarios. Not because they have become more probable—no one knows how likely they are—but because policymakers around the world are mulling measures to guard against them. The European Union is finalising an expansive <small>AI</small> act; the White House is expected soon to issue an executive order aimed at <small>llm</small>s; and on November 1st and 2nd the British government will convene world leaders and tech bosses for an “<a href="https://www.economist.com/business/2023/10/24/the-world-wants-to-regulate-ai-but-does-not-quite-know-how"><small>AI</small> Safety Summit</a>” to discuss the extreme risks that <small>AI </small>models may pose.</p><div class="adComponent_advert__V79Pp adComponent_incontent__Bxd2J adComponent_hidden___o_ZB css-0 e1cbnkh80"><div><div class="adComponent_adcontainer__dO1Zm" id="econ-1"></div></div></div><p class="css-1hno3qs e190yofl0" data-component="paragraph">Governments cannot ignore a technology that could change the world profoundly, and any credible threat to humanity should be taken seriously. Regulators have been too slow in the past. Many wish they had acted faster to police social media in the 2010s, and are keen to be on the front foot this time. But there is danger, too, in acting hastily. If they go too fast, policymakers could create global rules and institutions that are aimed at the wrong problems, are ineffective against the real ones and which stifle innovation.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">The idea that <small>AI</small> could drive humanity to extinction is still entirely speculative. No one yet knows how such a threat might materialise. No common methods exist to establish what counts as risky, much less to evaluate models against a benchmark for danger. Plenty of research needs to be done before standards and rules can be set. This is why a growing number of tech executives say the world needs a body to study <small>AI</small> much like the Intergovernmental Panel on Climate Change (<small>IPCC</small>), which tracks and explains global warming.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">A rush to regulate away tail risks could distract policymakers from less apocalyptic but more pressing problems. New laws may be needed to govern the use of copyrighted materials when training <small>LLM</small>s, or to define privacy rights as models guzzle personal data. And <small>ai</small> will make it much easier to produce disinformation, a thorny problem for every society.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Hasty regulation could also stifle competition and innovation. Because of the computing resources and technical skills required, only a handful of companies have so far developed powerful “frontier” models. New regulation could easily entrench the incumbents and block out competitors, not least because the biggest model-makers are working closely with governments on writing the rule book. A focus on extreme risks is likely to make regulators wary of open-source models, which are freely available and can easily be modified; until recently the White House was rumoured to be considering banning firms from releasing frontier open-source models. Yet if those risks do not materialise, restraining open-source models would serve only to limit an important source of competition.</p><div class="adComponent_advert__V79Pp adComponent_incontent__Bxd2J adComponent_hidden___o_ZB css-0 e1cbnkh80"><div><div class="adComponent_adcontainer__dO1Zm" id="econ-2"></div></div></div><p class="css-1hno3qs e190yofl0" data-component="paragraph">Regulators must be prepared to react quickly if needed, but should not be rushed into setting rules or building institutions that turn out to be unnecessary or harmful. Too little is known about the direction of generative <small>AI</small> to understand the risks associated with it, let alone manage them.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">The best that governments can do now is to set up the infrastructure to study the technology and its potential perils, and ensure that those working on the problem have adequate resources. In today’s fractious world, it will be hard to establish an <small>IPCC</small>-like body, and for it to thrive. But bodies that already work on <small>AI</small>-related questions, such as the <small>OECD</small> and Britain’s newish Frontier <small>AI</small> Taskforce, which aims to gain access to models’ nuts and bolts, could work closely together.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">It would help if governments agreed to a code of conduct for model-makers, much like the “voluntary commitments” negotiated by the White House and to which 15 makers of proprietary models have already signed up. These oblige model-makers, among other things, to share information about how they are managing <small>AI</small> risk. Though the commitments are not binding, they may help avoid a dangerous free-for-all. Makers of open-source models, too, should be urged to join up.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">As <small>AI</small> develops further, regulators will have a far better idea of what risks they are guarding against, and consequently what the rule book should look like. A fully fledged regime could eventually look rather like those for other technologies of world-changing import, such as nuclear power or bioengineering. But creating it will take time—and deliberation. <span class="ufinish">■</span></p></div></section></body>
    <script>
    window.tedl = window.tedl || {};
    // Resize iframes on articles with interactives when they send a RESIZE message
    window.addEventListener('message', (event) => {
    if (event.data.type === 'RESIZE') {
    Array.prototype.forEach.call(document.getElementsByTagName('iframe'), function (element) {
    if (element.contentWindow === event.source) {
    const height = parseInt(event.data.payload.height, 10);
    const elementHeight = parseInt(element.style.height, 10);
    if (isNaN(elementHeight) | Math.abs(elementHeight - height) > 10){
        element.style.height = height + 'px';
    }
    // 
    console.log(elementHeight - height);
    }
    });
    }
    }, false);
    </script>
    </html>