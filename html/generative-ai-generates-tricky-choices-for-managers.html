<html lang="en"><meta name="viewport" content="width=device-width, initial-scale=1" /><head><link rel="stylesheet" href="../init.css"></head><body><section class="css-qm1vln e1m3q8rc0"><div class="css-b4a1pi elvs37n0"><span class="css-1te5c83 e11l78dl0"><a data-analytics="sidebar:section" href="/business/"><span>Business</span></a></span><span class="css-1mdqtqm e11bnqe00"> | <!-- -->Bartleby</span></div><h1 class="css-fk78xg e1t4u3eq0">Generative AI generates tricky choices for managers </h1><h2 class="css-o55d57 elmr55g0">Transformational technologies can be very trying </h2></section><img id="myImg" src="../image/20231202_WBD002.jpg" width="auto" height="200"><section class="css-1fpllpa ee5d8yd1" data-body-id="cp2"><div class="css-knmu16 ee5d8yd2"><div class="css-1lm38nn e50l3u40"><figure><div><figcaption>Listen to this story.</figcaption> <span class="css-1fs0t47 ed4dtdz0">Enjoy more audio and podcasts on<!-- --> <a href="https://economist-app.onelink.me/d2eC/bed1b25" id="audio-ios-cta" rel="noreferrer" target="_blank">iOS</a> <!-- -->or<!-- --> <a href="https://economist-app.onelink.me/d2eC/7f3c199" id="audio-android-cta" rel="noreferrer" target="_blank">Android</a>.</span></div><audio class="react-audio-player" controls="" controlslist="nodownload" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/059%20Business%20-%20Bartleby-ebd8f36b64129e25c9511f55fab5fcd3.mp3" title="Generative AI generates tricky choices for managers "><p>Your browser does not support the &lt;audio&gt; element.</p></audio><div class="css-1h1me4y e50l3u41"><div class="css-vu6x35 ebr52qc0"></div></div></figure></div><p class="css-1hno3qs e190yofl0" data-component="paragraph"><span data-caps="initial">T</span><small>he remarkable</small> capabilities of generative <a href="https://www.economist.com/artificial-intelligence">artificial intelligence</a> (<small>AI</small>) are clear the moment you try it. But remarkableness is also a <a href="https://www.economist.com/business/2023/10/24/pity-the-modern-manager-burnt-out-distracted-and-overloaded">problem for managers</a>. Working out what to do with a new technology is harder when it can affect so many activities; when its adoption depends not just on the abilities of machines but also on pesky humans; and when it has some surprising flaws.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Study after study rams home the potential of large language models (<small>LLM</small>s), which power <small>AI</small>s like Chat<small>GPT</small>, to improve all manner of things. <small>LLM</small>s can save time, by generating meeting summaries, analysing data or drafting press releases. They can sharpen up customer service. They cannot put up <small>IKEA</small> bookshelves—but nor can humans.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph"><small>AI</small> can even boost innovation. Karan Girotra of Cornell University and his co-authors compared the idea-generating abilities of the latest version of Chat<small>GPT</small> with those of students at an elite university. A lone human can come up with about five ideas in 15 minutes; arm the human with the <small>AI</small> and the number goes up to 200. Crucially, the quality of these ideas is better, at least judged by purchase-intent surveys for new product concepts. Such possibilities can paralyse bosses; when you can do everything, it’s easy to do nothing.</p><aside class="css-13k2iu7 e1e9ys590" data-component="infobox"><p class="css-1hno3qs e190yofl0" data-component="paragraph"><i>In our new seven-part podcast series, <a href="https://www.economist.com/podcasts/2023/10/02/introducing-boss-class-from-the-economist">Boss Class</a>, our Bartleby columnist searches for the secrets to being a better manager.  <a href="https://www.economist.com/podcasts/2023/11/20/6-into-the-upside-down">Episode six</a> looks at how to motivate staff and <a href="https://www.economist.com/podcasts/2023/11/27/7-human-factors">episode seven</a> asks how managers should manage themselves.</i></p></aside><p class="css-1hno3qs e190yofl0" data-component="paragraph"><small>LLM</small>s’ ease of use also has pluses and minuses. On the plus side, more applications for generative <small>AI</small> can be found if more people are trying it. Familiarity with <small>LLM</small>s will make people better at using them. Reid Hoffman, a serial <small>AI</small> investor (and a guest on this week’s final episode of “Boss Class”, our management podcast), has a simple bit of advice: start playing with it. If you asked Chat<small>GPT</small> to write a haiku a year ago and have not touched it since, you have more to do.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Familiarity may also counter the human instinct to be wary of automation. A paper by Siliang Tong of Nanyang Technological University and his co-authors that was published in 2021, before generative <small>AI</small> was all the rage, captured this suspicion neatly. It showed that <small>AI</small>-generated feedback improved employee performance more than feedback from human managers. However, disclosing that the feedback came from a machine had the opposite effect: it undermined trust, stoked fears of job insecurity and hurt performance. Exposure to <small>LLM</small>s could soothe concerns.</p><div class="adComponent_advert__V79Pp adComponent_incontent__Bxd2J adComponent_hidden___o_ZB css-0 e1cbnkh80"><div><div class="adComponent_adcontainer__dO1Zm" id="econ-1"></div></div></div><p class="css-1hno3qs e190yofl0" data-component="paragraph">Or not. Complicating things are flaws in the technology. The Cambridge Dictionary has named “hallucinate” as its word of the year, in tribute to the tendency of <small>LLM</small>s to spew out false information. The models are evolving rapidly and ought to get better on this score, at least. But some problems are baked in, according to a new paper by R. Thomas McCoy of Princeton University and his co-authors.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">Because off-the-shelf models are trained on internet data to predict the next word in an answer on a probabilistic basis, they can be tripped up by surprising things. Get <small>GPT</small>-4, the <small>LLM</small> behind Chat<small>GPT</small>, to multiply a number by 9/5 and add 32, and it does well; ask it to multiply the same number by 7/5 and add 31, and it does considerably less well. The difference is explained by the fact that the first calculation is how you convert Celsius to Fahrenheit, and therefore common on the internet; the second is rare and so does not feature much in the training data. Such pitfalls will exist in proprietary models, too.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">On top of all this is a practical problem: it is hard for firms to keep track of employees’ use of <small>AI</small>. Confidential data might be uploaded and potentially leak out in a subsequent conversation. Earlier this year Samsung, an electronics giant, clamped down on usage of Chat<small>GPT</small> by employees after engineers reportedly shared source code with the chatbot.</p><p class="css-1hno3qs e190yofl0" data-component="paragraph">This combination of superpowers, simplicity and stumbles is a messy one for bosses to navigate. But it points to a few rules of thumb. Be targeted. Some consultants like to talk about the “lighthouse approach”—picking a contained project that has signalling value to the rest of the organisation. Rather than banning the use of <small>LLM</small>s, have guidelines on what information can be put into them. Be on top of how the tech works: this is not like driving a car and not caring what is under the hood. Above all, use it yourself. Generative <small>AI</small> may feel magical. But it is hard work to get right.<span class="ufinish">■</span></p><div class="adComponent_advert__V79Pp adComponent_incontent__Bxd2J adComponent_hidden___o_ZB css-0 e1cbnkh80"><div><div class="adComponent_adcontainer__dO1Zm" id="econ-2"></div></div></div><p class="css-1hno3qs e190yofl0" data-component="paragraph"><i><b>Correction (28th November): </b>An earlier version of this article stated that the study by Karan Girotra and his co-authors took place at several elite American universities. It actually took place at just one elite university. It also stated that R. Thomas McCoy’s co-authors are also at Princeton University. Not all of them still are. Apologies.</i></p><p class="css-1hno3qs e190yofl0" data-component="paragraph"><b>Read more from Bartleby, our columnist on management and work:<br/></b><i><a href="https://www.economist.com/business/2023/11/20/how-not-to-motivate-your-employees">How not to motivate your employees</a> (Nov 20th)<br/></i><i><a href="https://www.economist.com/business/article76257-prod.ece">The curse of the badly run meeting</a> (Nov 13th)<br/></i><i><a href="https://www.economist.com/business/2023/11/06/how-to-manage-teams-in-a-world-designed-for-individuals">How to manage teams in a world designed for individuals</a> (Nov 6th)</i></p><p class="css-1hno3qs e190yofl0" data-component="paragraph"><i>Also: How the Bartleby column <a href="https://www.economist.com/column-names">got its name</a></i></p></div></section></body>
    <script>
    window.tedl = window.tedl || {};
    // Resize iframes on articles with interactives when they send a RESIZE message
    window.addEventListener('message', (event) => {
    if (event.data.type === 'RESIZE') {
    Array.prototype.forEach.call(document.getElementsByTagName('iframe'), function (element) {
    if (element.contentWindow === event.source) {
    const height = parseInt(event.data.payload.height, 10);
    const elementHeight = parseInt(element.style.height, 10);
    if (isNaN(elementHeight) | Math.abs(elementHeight - height) > 10){
        element.style.height = height + 'px';
    }
    // 
    console.log(elementHeight - height);
    }
    });
    }
    }, false);
    </script>
    </html>